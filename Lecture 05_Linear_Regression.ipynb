{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerallen741/PyTorch-Learning/blob/master/Lecture%2005_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENnHzOw_BQou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = torch.Tensor([[2.0], [4.0], [6.0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxdUj443DNri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define Model\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.linear = torch.nn.Linear(1,1)  # One in and one out\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ98Wl5wEfPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model\n",
        "model = Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGhfktnoGaq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Construct loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHRXzcn-HK_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4d039d0-60f1-42c5-f895-bc93352f28f4"
      },
      "source": [
        "#Training loop\n",
        "for epoch in range(300):\n",
        "  # 1) Forward pass: computer predicited y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "  # 2) Computer and print loss\n",
        "  loss = criterion(y_pred,y_data)\n",
        "  print('Epoch:{:d}, loss:{:f}'.format(epoch,loss.item()))\n",
        "  \n",
        "  # 3) Zero gradients, perform a backward pass, and update the weights.\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0, loss:8.868632\n",
            "Epoch:1, loss:4.133110\n",
            "Epoch:2, loss:2.022331\n",
            "Epoch:3, loss:1.080051\n",
            "Epoch:4, loss:0.657991\n",
            "Epoch:5, loss:0.467556\n",
            "Epoch:6, loss:0.380269\n",
            "Epoch:7, loss:0.338938\n",
            "Epoch:8, loss:0.318101\n",
            "Epoch:9, loss:0.306421\n",
            "Epoch:10, loss:0.298853\n",
            "Epoch:11, loss:0.293150\n",
            "Epoch:12, loss:0.288309\n",
            "Epoch:13, loss:0.283887\n",
            "Epoch:14, loss:0.279682\n",
            "Epoch:15, loss:0.275608\n",
            "Epoch:16, loss:0.271622\n",
            "Epoch:17, loss:0.267707\n",
            "Epoch:18, loss:0.263855\n",
            "Epoch:19, loss:0.260061\n",
            "Epoch:20, loss:0.256323\n",
            "Epoch:21, loss:0.252638\n",
            "Epoch:22, loss:0.249007\n",
            "Epoch:23, loss:0.245429\n",
            "Epoch:24, loss:0.241902\n",
            "Epoch:25, loss:0.238425\n",
            "Epoch:26, loss:0.234998\n",
            "Epoch:27, loss:0.231621\n",
            "Epoch:28, loss:0.228292\n",
            "Epoch:29, loss:0.225012\n",
            "Epoch:30, loss:0.221778\n",
            "Epoch:31, loss:0.218591\n",
            "Epoch:32, loss:0.215449\n",
            "Epoch:33, loss:0.212353\n",
            "Epoch:34, loss:0.209301\n",
            "Epoch:35, loss:0.206293\n",
            "Epoch:36, loss:0.203328\n",
            "Epoch:37, loss:0.200406\n",
            "Epoch:38, loss:0.197526\n",
            "Epoch:39, loss:0.194687\n",
            "Epoch:40, loss:0.191889\n",
            "Epoch:41, loss:0.189131\n",
            "Epoch:42, loss:0.186413\n",
            "Epoch:43, loss:0.183734\n",
            "Epoch:44, loss:0.181094\n",
            "Epoch:45, loss:0.178491\n",
            "Epoch:46, loss:0.175926\n",
            "Epoch:47, loss:0.173398\n",
            "Epoch:48, loss:0.170906\n",
            "Epoch:49, loss:0.168449\n",
            "Epoch:50, loss:0.166028\n",
            "Epoch:51, loss:0.163642\n",
            "Epoch:52, loss:0.161290\n",
            "Epoch:53, loss:0.158973\n",
            "Epoch:54, loss:0.156688\n",
            "Epoch:55, loss:0.154436\n",
            "Epoch:56, loss:0.152216\n",
            "Epoch:57, loss:0.150029\n",
            "Epoch:58, loss:0.147873\n",
            "Epoch:59, loss:0.145747\n",
            "Epoch:60, loss:0.143653\n",
            "Epoch:61, loss:0.141588\n",
            "Epoch:62, loss:0.139554\n",
            "Epoch:63, loss:0.137548\n",
            "Epoch:64, loss:0.135571\n",
            "Epoch:65, loss:0.133623\n",
            "Epoch:66, loss:0.131702\n",
            "Epoch:67, loss:0.129810\n",
            "Epoch:68, loss:0.127944\n",
            "Epoch:69, loss:0.126105\n",
            "Epoch:70, loss:0.124293\n",
            "Epoch:71, loss:0.122507\n",
            "Epoch:72, loss:0.120746\n",
            "Epoch:73, loss:0.119011\n",
            "Epoch:74, loss:0.117300\n",
            "Epoch:75, loss:0.115615\n",
            "Epoch:76, loss:0.113953\n",
            "Epoch:77, loss:0.112315\n",
            "Epoch:78, loss:0.110701\n",
            "Epoch:79, loss:0.109110\n",
            "Epoch:80, loss:0.107542\n",
            "Epoch:81, loss:0.105997\n",
            "Epoch:82, loss:0.104473\n",
            "Epoch:83, loss:0.102972\n",
            "Epoch:84, loss:0.101492\n",
            "Epoch:85, loss:0.100034\n",
            "Epoch:86, loss:0.098596\n",
            "Epoch:87, loss:0.097179\n",
            "Epoch:88, loss:0.095782\n",
            "Epoch:89, loss:0.094406\n",
            "Epoch:90, loss:0.093049\n",
            "Epoch:91, loss:0.091712\n",
            "Epoch:92, loss:0.090394\n",
            "Epoch:93, loss:0.089094\n",
            "Epoch:94, loss:0.087814\n",
            "Epoch:95, loss:0.086552\n",
            "Epoch:96, loss:0.085308\n",
            "Epoch:97, loss:0.084082\n",
            "Epoch:98, loss:0.082874\n",
            "Epoch:99, loss:0.081683\n",
            "Epoch:100, loss:0.080509\n",
            "Epoch:101, loss:0.079352\n",
            "Epoch:102, loss:0.078211\n",
            "Epoch:103, loss:0.077087\n",
            "Epoch:104, loss:0.075980\n",
            "Epoch:105, loss:0.074888\n",
            "Epoch:106, loss:0.073811\n",
            "Epoch:107, loss:0.072750\n",
            "Epoch:108, loss:0.071705\n",
            "Epoch:109, loss:0.070675\n",
            "Epoch:110, loss:0.069659\n",
            "Epoch:111, loss:0.068658\n",
            "Epoch:112, loss:0.067671\n",
            "Epoch:113, loss:0.066698\n",
            "Epoch:114, loss:0.065740\n",
            "Epoch:115, loss:0.064795\n",
            "Epoch:116, loss:0.063864\n",
            "Epoch:117, loss:0.062946\n",
            "Epoch:118, loss:0.062041\n",
            "Epoch:119, loss:0.061150\n",
            "Epoch:120, loss:0.060271\n",
            "Epoch:121, loss:0.059405\n",
            "Epoch:122, loss:0.058551\n",
            "Epoch:123, loss:0.057709\n",
            "Epoch:124, loss:0.056880\n",
            "Epoch:125, loss:0.056063\n",
            "Epoch:126, loss:0.055257\n",
            "Epoch:127, loss:0.054463\n",
            "Epoch:128, loss:0.053680\n",
            "Epoch:129, loss:0.052909\n",
            "Epoch:130, loss:0.052148\n",
            "Epoch:131, loss:0.051399\n",
            "Epoch:132, loss:0.050660\n",
            "Epoch:133, loss:0.049932\n",
            "Epoch:134, loss:0.049215\n",
            "Epoch:135, loss:0.048507\n",
            "Epoch:136, loss:0.047810\n",
            "Epoch:137, loss:0.047123\n",
            "Epoch:138, loss:0.046446\n",
            "Epoch:139, loss:0.045778\n",
            "Epoch:140, loss:0.045120\n",
            "Epoch:141, loss:0.044472\n",
            "Epoch:142, loss:0.043833\n",
            "Epoch:143, loss:0.043203\n",
            "Epoch:144, loss:0.042582\n",
            "Epoch:145, loss:0.041970\n",
            "Epoch:146, loss:0.041367\n",
            "Epoch:147, loss:0.040772\n",
            "Epoch:148, loss:0.040186\n",
            "Epoch:149, loss:0.039609\n",
            "Epoch:150, loss:0.039040\n",
            "Epoch:151, loss:0.038478\n",
            "Epoch:152, loss:0.037926\n",
            "Epoch:153, loss:0.037380\n",
            "Epoch:154, loss:0.036843\n",
            "Epoch:155, loss:0.036314\n",
            "Epoch:156, loss:0.035792\n",
            "Epoch:157, loss:0.035277\n",
            "Epoch:158, loss:0.034770\n",
            "Epoch:159, loss:0.034271\n",
            "Epoch:160, loss:0.033778\n",
            "Epoch:161, loss:0.033293\n",
            "Epoch:162, loss:0.032814\n",
            "Epoch:163, loss:0.032343\n",
            "Epoch:164, loss:0.031878\n",
            "Epoch:165, loss:0.031420\n",
            "Epoch:166, loss:0.030968\n",
            "Epoch:167, loss:0.030523\n",
            "Epoch:168, loss:0.030084\n",
            "Epoch:169, loss:0.029652\n",
            "Epoch:170, loss:0.029226\n",
            "Epoch:171, loss:0.028806\n",
            "Epoch:172, loss:0.028392\n",
            "Epoch:173, loss:0.027984\n",
            "Epoch:174, loss:0.027582\n",
            "Epoch:175, loss:0.027185\n",
            "Epoch:176, loss:0.026795\n",
            "Epoch:177, loss:0.026410\n",
            "Epoch:178, loss:0.026030\n",
            "Epoch:179, loss:0.025656\n",
            "Epoch:180, loss:0.025287\n",
            "Epoch:181, loss:0.024924\n",
            "Epoch:182, loss:0.024566\n",
            "Epoch:183, loss:0.024212\n",
            "Epoch:184, loss:0.023865\n",
            "Epoch:185, loss:0.023522\n",
            "Epoch:186, loss:0.023184\n",
            "Epoch:187, loss:0.022850\n",
            "Epoch:188, loss:0.022522\n",
            "Epoch:189, loss:0.022198\n",
            "Epoch:190, loss:0.021879\n",
            "Epoch:191, loss:0.021565\n",
            "Epoch:192, loss:0.021255\n",
            "Epoch:193, loss:0.020950\n",
            "Epoch:194, loss:0.020648\n",
            "Epoch:195, loss:0.020352\n",
            "Epoch:196, loss:0.020059\n",
            "Epoch:197, loss:0.019771\n",
            "Epoch:198, loss:0.019487\n",
            "Epoch:199, loss:0.019207\n",
            "Epoch:200, loss:0.018931\n",
            "Epoch:201, loss:0.018659\n",
            "Epoch:202, loss:0.018390\n",
            "Epoch:203, loss:0.018126\n",
            "Epoch:204, loss:0.017866\n",
            "Epoch:205, loss:0.017609\n",
            "Epoch:206, loss:0.017356\n",
            "Epoch:207, loss:0.017106\n",
            "Epoch:208, loss:0.016861\n",
            "Epoch:209, loss:0.016618\n",
            "Epoch:210, loss:0.016379\n",
            "Epoch:211, loss:0.016144\n",
            "Epoch:212, loss:0.015912\n",
            "Epoch:213, loss:0.015683\n",
            "Epoch:214, loss:0.015458\n",
            "Epoch:215, loss:0.015236\n",
            "Epoch:216, loss:0.015017\n",
            "Epoch:217, loss:0.014801\n",
            "Epoch:218, loss:0.014588\n",
            "Epoch:219, loss:0.014379\n",
            "Epoch:220, loss:0.014172\n",
            "Epoch:221, loss:0.013968\n",
            "Epoch:222, loss:0.013768\n",
            "Epoch:223, loss:0.013570\n",
            "Epoch:224, loss:0.013375\n",
            "Epoch:225, loss:0.013182\n",
            "Epoch:226, loss:0.012993\n",
            "Epoch:227, loss:0.012806\n",
            "Epoch:228, loss:0.012622\n",
            "Epoch:229, loss:0.012441\n",
            "Epoch:230, loss:0.012262\n",
            "Epoch:231, loss:0.012086\n",
            "Epoch:232, loss:0.011912\n",
            "Epoch:233, loss:0.011741\n",
            "Epoch:234, loss:0.011572\n",
            "Epoch:235, loss:0.011406\n",
            "Epoch:236, loss:0.011242\n",
            "Epoch:237, loss:0.011080\n",
            "Epoch:238, loss:0.010921\n",
            "Epoch:239, loss:0.010764\n",
            "Epoch:240, loss:0.010610\n",
            "Epoch:241, loss:0.010457\n",
            "Epoch:242, loss:0.010307\n",
            "Epoch:243, loss:0.010159\n",
            "Epoch:244, loss:0.010013\n",
            "Epoch:245, loss:0.009869\n",
            "Epoch:246, loss:0.009727\n",
            "Epoch:247, loss:0.009587\n",
            "Epoch:248, loss:0.009449\n",
            "Epoch:249, loss:0.009314\n",
            "Epoch:250, loss:0.009180\n",
            "Epoch:251, loss:0.009048\n",
            "Epoch:252, loss:0.008918\n",
            "Epoch:253, loss:0.008790\n",
            "Epoch:254, loss:0.008663\n",
            "Epoch:255, loss:0.008539\n",
            "Epoch:256, loss:0.008416\n",
            "Epoch:257, loss:0.008295\n",
            "Epoch:258, loss:0.008176\n",
            "Epoch:259, loss:0.008058\n",
            "Epoch:260, loss:0.007943\n",
            "Epoch:261, loss:0.007828\n",
            "Epoch:262, loss:0.007716\n",
            "Epoch:263, loss:0.007605\n",
            "Epoch:264, loss:0.007496\n",
            "Epoch:265, loss:0.007388\n",
            "Epoch:266, loss:0.007282\n",
            "Epoch:267, loss:0.007177\n",
            "Epoch:268, loss:0.007074\n",
            "Epoch:269, loss:0.006972\n",
            "Epoch:270, loss:0.006872\n",
            "Epoch:271, loss:0.006773\n",
            "Epoch:272, loss:0.006676\n",
            "Epoch:273, loss:0.006580\n",
            "Epoch:274, loss:0.006485\n",
            "Epoch:275, loss:0.006392\n",
            "Epoch:276, loss:0.006300\n",
            "Epoch:277, loss:0.006210\n",
            "Epoch:278, loss:0.006121\n",
            "Epoch:279, loss:0.006033\n",
            "Epoch:280, loss:0.005946\n",
            "Epoch:281, loss:0.005861\n",
            "Epoch:282, loss:0.005776\n",
            "Epoch:283, loss:0.005693\n",
            "Epoch:284, loss:0.005611\n",
            "Epoch:285, loss:0.005531\n",
            "Epoch:286, loss:0.005451\n",
            "Epoch:287, loss:0.005373\n",
            "Epoch:288, loss:0.005296\n",
            "Epoch:289, loss:0.005220\n",
            "Epoch:290, loss:0.005145\n",
            "Epoch:291, loss:0.005071\n",
            "Epoch:292, loss:0.004998\n",
            "Epoch:293, loss:0.004926\n",
            "Epoch:294, loss:0.004855\n",
            "Epoch:295, loss:0.004785\n",
            "Epoch:296, loss:0.004717\n",
            "Epoch:297, loss:0.004649\n",
            "Epoch:298, loss:0.004582\n",
            "Epoch:299, loss:0.004516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XlgYMz-JURR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f026dfbf-4b01-41e5-b8dd-fe6be34fb4ef"
      },
      "source": [
        "#After training\n",
        "test_var = tensor([[4.0]])\n",
        "y_pred = model(test_var)\n",
        "print(\"Prediction (after training)\",  4, model(test_var).data[0][0].item())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction (after training) 4 7.922747611999512\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
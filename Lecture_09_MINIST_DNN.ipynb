{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 09 MINIST_DNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerallen741/PyTorch-Learning/blob/master/Lecture_09_MINIST_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j34sds1utdMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbO3CU8OvkSe",
        "colab_type": "code",
        "outputId": "8f2a1457-a69a-4e65-c621-8b346344e9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Training settings\n",
        "batch_size = 64\n",
        "lr_rate = 0.01\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST　Model on {device}\\n{\"=\" * 44}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MNIST　Model on cpu\n",
            "============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O32JDkLsv8-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFn8rsnIuKlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.l1 = nn.Linear(784,520)\n",
        "    self.l2 = nn.Linear(520,320)\n",
        "    self.l3 = nn.Linear(320,240)\n",
        "    self.l4 = nn.Linear(240,120)\n",
        "    self.l5 = nn.Linear(120,10)\n",
        "  def forward(self, x):\n",
        "    #Flatten the data(n,1,28,28) -> (n,784)\n",
        "    x = x.view(-1,784)\n",
        "    x = nn.functional .relu(self.l1(x))\n",
        "    x = nn.functional .relu(self.l2(x))\n",
        "    x = nn.functional .relu(self.l3(x))\n",
        "    x = nn.functional .relu(self.l4(x))\n",
        "    return nn.functional .relu(self.l5(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmyWAyTJx03C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr_rate, momentum=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUtLitqHyNIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYEM4qazhrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    # sum up batch loss \n",
        "    test_loss += criterion(output, target).item()   \n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)  \n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNhYVLpM0gu9",
        "colab_type": "code",
        "outputId": "84781989-f664-4716-cf0c-9d93efed9cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 2):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.292895\n",
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.300655\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.307607\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.298260\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.306551\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.311967\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.301858\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.305346\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.300320\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.289805\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.300591\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.299995\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.296573\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.300665\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.298820\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.301942\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.301161\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.291597\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.294674\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.306541\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.291675\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.281339\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.298048\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.301924\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.298173\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.291889\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.301084\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.304573\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.284817\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.292996\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.295286\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.302108\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.295314\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.288009\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.293026\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.288401\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.297458\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.294755\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.290664\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.296554\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.294648\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.300616\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.281865\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.280555\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.297815\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.290141\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.289471\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.290673\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.285574\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.286428\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.290208\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.274561\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.283576\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.287642\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.293551\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.288218\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.281785\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.289913\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.285670\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.289564\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.293647\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.276666\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.280201\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.266883\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.290182\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.268051\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.274912\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.258162\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.274777\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.265346\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.262335\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.264776\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.260044\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.269560\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.248003\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.252887\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.261637\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.246874\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.274265\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.233284\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.228775\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.223038\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.227441\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.227493\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 2.243095\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 2.218828\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 2.214677\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 2.201746\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 2.225329\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 2.222600\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 2.181468\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 2.160916\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 2.184939\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 2.170521\n",
            "Training time: 0m 14s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 23/10000 (0%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 52/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 81/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 107/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 129/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 155/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 185/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 210/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 232/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 256/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 285/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 308/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 342/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 373/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 401/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 432/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 459/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 484/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 512/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 526/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 549/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 582/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 597/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 621/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 646/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 670/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 691/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 716/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 747/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 771/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 801/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 829/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 855/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 883/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 905/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 937/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 968/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 990/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1015/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1039/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1064/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1085/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1115/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1139/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1163/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1183/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1209/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1235/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1268/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1291/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1320/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1345/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1371/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1399/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1420/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1440/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1469/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1494/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1517/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1537/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1550/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1579/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1608/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1631/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1659/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1687/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1707/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1729/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1747/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1766/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1795/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1830/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1853/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1888/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1913/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1932/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1960/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1986/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2012/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2042/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2069/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2095/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2125/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2159/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2186/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2219/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2253/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2283/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2317/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2347/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2379/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2409/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2441/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2467/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2502/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2534/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2563/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2595/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2628/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2662/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2689/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2716/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2737/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2767/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2798/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2821/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2849/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2877/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2903/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2935/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2962/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2995/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3025/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3052/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3089/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3116/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3144/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3170/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3200/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3230/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3262/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3292/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3312/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3338/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3370/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3395/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3419/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3450/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3474/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3506/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3536/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3571/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3602/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3636/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3669/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3699/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3731/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3762/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3792/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3823/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3858/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3889/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3922/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3953/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3983/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4013/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4045/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4076/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4108/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4140/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4168/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4196/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4223/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4251/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4273/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4302/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4310/10000 (43%)\n",
            "Testing time: 0m 15s\n",
            "Total Time: 0m 15s\n",
            "Model was trained on cpu!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture_10_MINIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerallen741/PyTorch-Learning/blob/master/Lecture_10_MINIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoTwcFclQyCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_1yc0wSQ1JU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c688e660-b1b5-40f9-f3f4-ffe6faaead14"
      },
      "source": [
        "#Training settings\n",
        "batch_size = 64\n",
        "lr_rate = 0.01\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST　Model on {device}\\n{\"=\" * 44}')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MNIST　Model on cpu\n",
            "============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYva4ddeQ2J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9uzdgtJ_AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.fc = nn.Linear(320, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0)\n",
        "    x = nn.functional.relu(self.mp(self.conv1(x)))  \n",
        "    x = nn.functional.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1) # flatten the tensor\n",
        "    \n",
        "    x = self.fc(x)\n",
        "    return  nn.functional.softmax(x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZtnKVk9Tbev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr_rate, momentum=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8YOjA8AThHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx*len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3qFVBqGUkMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data,volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    # sum up batch loss \n",
        "    test_loss += criterion(output, target).item()   \n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)  \n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yD5x1nsWeHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "685386fb-fcf8-421f-b0ba-a08e731f1d8e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 5):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        \n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.304322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.302756\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.303360\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.302276\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.300730\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.301801\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.300331\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.302675\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.302205\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.302462\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.301515\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.300725\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.300614\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.301218\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.302459\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.300503\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.301898\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.298385\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.301257\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.300841\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.302546\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.302023\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.296936\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.299560\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.301821\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.299811\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.302828\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.296026\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.300761\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.302522\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.299505\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.298408\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.297751\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.298513\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.298531\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.299966\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.297921\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.298294\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.297950\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.298724\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.296554\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.296937\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.295928\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.296514\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.295396\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.296558\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.298945\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.301109\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.294509\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.301875\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.298157\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.298006\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.295588\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.293276\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.296169\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.290963\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.293414\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.298257\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.295449\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.291268\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.298516\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.288990\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.291296\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.292774\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.289034\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.289182\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.287914\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.291543\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.294018\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.282405\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.285954\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.282377\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.283518\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.278090\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.290034\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.281220\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.267879\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.284839\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.271454\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.274763\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.272913\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.266478\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.264500\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.271195\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 2.260229\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 2.258522\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 2.241345\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 2.207145\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 2.232557\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 2.239400\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 2.230009\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 2.196989\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 2.227993\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 2.154489\n",
            "Training time: 0m 27s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 24/10000 (0%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 46/10000 (0%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 71/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 91/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 113/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 138/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 157/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 175/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 192/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 211/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 237/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 259/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 279/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 299/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 316/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 338/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 354/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 374/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 398/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 415/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 431/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 456/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 469/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 489/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 510/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 528/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 549/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 572/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 595/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 617/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 636/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 659/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 677/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 694/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 716/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 738/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 768/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 790/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 814/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 831/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 851/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 872/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 895/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 915/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 938/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 952/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 974/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 997/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1018/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1039/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1065/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1092/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1117/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1146/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1159/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1175/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1202/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1225/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1246/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1260/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1278/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1293/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1320/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1350/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1369/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1393/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1410/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1430/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1444/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1462/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1482/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1509/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1531/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1554/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1578/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1597/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1619/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1639/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1663/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1684/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1704/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1722/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1750/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1779/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1799/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1825/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1853/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1872/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1903/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1924/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1945/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1970/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1997/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2025/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2048/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2077/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2097/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2122/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2146/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2168/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2190/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2210/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2230/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2252/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2278/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2305/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2333/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2353/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2382/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2402/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2425/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2448/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2478/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2502/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2530/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2552/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2575/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2601/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2626/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2647/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2671/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2695/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2719/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2748/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2769/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2793/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2820/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2839/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2860/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2885/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2911/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2938/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2962/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2989/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3011/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3033/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3058/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3082/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3110/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3129/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3155/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3182/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3206/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3231/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3260/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3290/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3315/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3343/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3363/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3383/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3412/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3432/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3452/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3480/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3504/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3532/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3537/10000 (35%)\n",
            "Testing time: 0m 29s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 2.175072\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 2.185392\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 2.195994\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 2.129636\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 2.066247\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 2.116871\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 2.119001\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 2.048932\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 2.043160\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 1.990748\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 1.962631\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 2.008218\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 1.998291\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 2.036883\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.865988\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 1.895419\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 1.882853\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 1.796342\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 1.860682\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 1.872357\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 1.914221\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 1.958693\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 1.793848\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 1.846444\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 1.759071\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 1.834363\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 1.732250\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 1.789622\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 1.858655\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 1.735926\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 1.897075\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 1.813970\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 1.733044\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 1.851906\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 1.757782\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 1.852969\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 1.773317\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 1.764510\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 1.795105\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 1.687283\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 1.752419\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 1.755593\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 1.768540\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 1.727099\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 1.710909\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 1.704820\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 1.741718\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 1.814901\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 1.697113\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 1.731174\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 1.644024\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 1.721775\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 1.745894\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 1.823229\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 1.728822\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 1.767683\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 1.645862\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 1.720435\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 1.711747\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 1.716197\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 1.693722\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 1.671833\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 1.687800\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 1.672666\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 1.612579\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 1.651575\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 1.700571\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 1.603916\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 1.593951\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 1.616001\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 1.679835\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 1.623281\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 1.662393\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 1.649858\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 1.664386\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 1.699615\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 1.751927\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 1.637820\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 1.632671\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 1.691567\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 1.623935\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 1.665494\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 1.705733\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 1.616492\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 1.656323\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 1.708083\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 1.712234\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 1.645432\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 1.679757\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 1.660927\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 1.689745\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 1.654083\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 1.685056\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 1.678986\n",
            "Training time: 0m 27s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 54/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 106/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 154/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 206/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 258/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 306/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 361/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 409/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 458/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 510/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 561/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 611/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 668/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 723/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 772/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 822/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 870/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 913/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 961/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1007/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1055/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1105/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1150/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1202/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1256/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1309/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1355/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1407/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1461/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1512/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1564/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1613/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1659/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1712/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1761/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1814/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1871/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1919/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1974/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2023/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2072/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2121/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2172/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2222/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2276/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2325/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2373/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2428/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2476/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2531/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2587/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2639/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2688/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2740/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2795/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2842/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2893/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2948/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2998/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3040/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3087/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3136/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3189/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3239/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3294/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3345/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3394/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3440/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3491/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3539/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3595/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3648/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3705/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3760/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3812/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3859/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3911/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3962/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4022/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4075/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4128/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4179/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4234/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4291/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4347/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4402/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4457/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4507/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4560/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4613/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4671/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4721/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4771/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4825/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4879/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4935/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4987/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5044/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5101/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5156/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5209/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5262/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5313/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5368/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5429/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5481/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5538/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5587/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5643/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5698/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5757/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5812/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5870/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5922/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5978/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6031/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6080/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6133/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6190/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6245/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6303/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6359/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6410/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6466/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6518/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6573/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6627/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6681/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6735/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6791/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6847/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6902/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6960/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7016/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7073/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7127/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7184/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7242/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7298/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7358/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7410/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7463/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7516/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7573/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7632/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7687/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7744/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7801/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7853/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7907/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7958/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8010/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8055/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8105/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8157/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8211/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8223/10000 (82%)\n",
            "Testing time: 0m 29s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 1.595999\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 1.741040\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 1.676621\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 1.656143\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 1.713046\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 1.709229\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 1.640908\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 1.598397\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 1.654267\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 1.546242\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 1.651560\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 1.643948\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 1.644394\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 1.651714\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 1.624737\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 1.682572\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 1.606626\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 1.628706\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 1.640380\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 1.619749\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 1.615885\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 1.642391\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 1.631880\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 1.661334\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 1.642171\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 1.618440\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 1.705994\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 1.661560\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 1.626431\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 1.614174\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 1.703936\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 1.655112\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 1.689987\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 1.597164\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 1.621191\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 1.714240\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 1.747421\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 1.619018\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 1.550321\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 1.658374\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 1.666298\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 1.600595\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 1.628371\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 1.596067\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 1.621732\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 1.662916\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 1.636614\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 1.602006\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 1.651471\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 1.651893\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 1.632826\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 1.680778\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 1.669002\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 1.604269\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 1.687398\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 1.714848\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 1.603896\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 1.578906\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 1.635645\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 1.602963\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 1.667818\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 1.648610\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 1.621064\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 1.582500\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 1.645619\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 1.715047\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 1.612839\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 1.512666\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 1.645977\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 1.631018\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 1.705077\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 1.580183\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 1.614492\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 1.621269\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 1.585725\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 1.574344\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 1.685180\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 1.569946\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 1.680527\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 1.689913\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 1.572804\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 1.624917\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 1.649395\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 1.644707\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 1.623987\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 1.644547\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 1.594915\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 1.616464\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 1.650245\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 1.605539\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 1.577827\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 1.559846\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 1.635092\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 1.718430\n",
            "Training time: 0m 26s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 55/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 111/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 163/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 215/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 268/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 317/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 372/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 423/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 476/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 528/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 580/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 632/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 690/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 746/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 797/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 849/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 901/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 947/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1000/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1047/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1098/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1152/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1203/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1255/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1311/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1365/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1413/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1467/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1524/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1577/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1631/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1682/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1733/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1785/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1838/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1893/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1951/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1999/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2056/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2107/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2156/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2206/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2259/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2310/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2365/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2417/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2466/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2524/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2574/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2629/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2689/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2744/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2796/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2849/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2906/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2956/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3009/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3066/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3118/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3161/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3208/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3257/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3312/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3364/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3420/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3470/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3521/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3569/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3624/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3674/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3731/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3786/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3845/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3900/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3956/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4007/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4061/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4114/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4173/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4226/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4279/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4332/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4387/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4444/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4501/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4559/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4614/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4664/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4717/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4770/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4829/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4879/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4931/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4986/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5039/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5094/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5146/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5203/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5261/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5316/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5371/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5425/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5477/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5531/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5593/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5649/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5705/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5756/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5812/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5867/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5926/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5981/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6040/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6096/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6153/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6209/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6259/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6314/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6371/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6425/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6484/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6541/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6592/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6649/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6704/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6760/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6815/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6870/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6927/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6986/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7044/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7101/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7159/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7216/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7274/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7328/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7386/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7444/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7500/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7561/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7613/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7666/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7724/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7780/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7839/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7894/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7951/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8010/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8061/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8116/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8170/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8222/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8270/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8325/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8376/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8432/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8445/10000 (84%)\n",
            "Testing time: 0m 29s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 1.617604\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 1.644164\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 1.608024\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 1.556699\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 1.652490\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 1.634010\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 1.680419\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 1.592299\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 1.583387\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 1.614117\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 1.632616\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 1.505749\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 1.576568\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 1.600135\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 1.627003\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 1.601903\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 1.596444\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 1.614617\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 1.677794\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 1.648319\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 1.618962\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 1.683556\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 1.639631\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 1.684022\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 1.654806\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 1.624462\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 1.599554\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 1.631772\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 1.591387\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 1.658696\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 1.587036\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 1.593734\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 1.624330\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 1.613308\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 1.653543\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 1.587073\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 1.642761\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 1.531103\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 1.595377\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 1.592992\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 1.582726\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 1.629700\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 1.658065\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 1.695927\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 1.625013\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 1.669388\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 1.610020\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 1.621567\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 1.581621\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 1.623708\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 1.589284\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 1.592626\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 1.746605\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 1.644697\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 1.625316\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 1.606574\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 1.564327\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 1.584273\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 1.640354\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 1.657798\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 1.629413\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 1.592153\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 1.557127\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 1.548073\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 1.625619\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 1.666320\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 1.601829\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 1.558128\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 1.661088\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 1.561679\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 1.664613\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 1.601944\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 1.637248\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 1.611232\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 1.658902\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 1.666821\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 1.583694\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 1.611450\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 1.655356\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 1.654869\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 1.592635\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 1.595045\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 1.649180\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 1.698418\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 1.602763\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 1.567574\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 1.682467\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 1.631687\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 1.618167\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 1.654527\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 1.591443\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 1.633228\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 1.592099\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 1.629867\n",
            "Training time: 0m 27s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 55/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 113/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 165/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 218/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 272/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 322/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 377/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 428/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 483/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 535/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 587/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 639/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 697/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 754/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 805/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 857/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 910/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 958/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1011/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1058/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1110/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1166/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1218/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1270/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1325/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1380/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1429/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1484/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1542/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1594/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1649/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1701/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1753/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1805/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1859/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1914/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1972/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2021/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2078/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2129/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2177/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2231/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2286/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2339/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2396/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2448/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2500/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2559/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2609/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2665/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2726/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2783/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2838/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2892/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2951/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3001/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3054/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3111/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3164/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3207/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3257/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3306/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3362/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3414/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3470/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3524/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3574/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3623/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3678/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3730/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3786/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3841/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3900/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3957/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4014/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4067/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4120/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4173/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4232/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4286/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4343/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4396/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4453/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4510/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4567/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4625/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4684/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4735/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4789/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4842/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4901/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4954/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5007/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5062/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5115/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5168/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5221/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5278/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5335/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5390/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5448/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5502/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5554/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5608/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0001, Accuracy: 5670/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5726/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5782/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5833/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5890/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5945/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6005/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6061/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6120/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6176/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6234/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6292/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6344/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6399/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6457/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6511/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6570/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6628/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6680/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6738/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6793/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6850/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6905/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6960/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7019/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7078/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7136/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7193/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7251/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7308/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7365/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7419/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7477/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7535/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7591/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7652/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7707/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7761/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7818/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7876/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7934/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7989/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8046/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8105/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8158/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8213/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8268/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8321/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8370/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8425/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8478/10000 (85%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8535/10000 (85%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8548/10000 (85%)\n",
            "Testing time: 0m 30s\n",
            "Total Time: 1m 57s\n",
            "Model was trained on cpu!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
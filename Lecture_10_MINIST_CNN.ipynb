{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture_10_MINIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerallen741/PyTorch-Learning/blob/master/Lecture_10_MINIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoTwcFclQyCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_1yc0wSQ1JU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training settings\n",
        "use_cuda = False\n",
        "batch_size = 64\n",
        "lr_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYva4ddeQ2J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9uzdgtJ_AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.fc = nn.Linear(320, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = nn.functional.relu(self.mp(self.conv1(x)))  \n",
        "    x = nn.functional.relu(self.mp(self.conv2(x)))\n",
        "    in_size = x.size()[0]\n",
        "    x = x.view(in_size, -1) # flatten the tensor\n",
        "    \n",
        "    #print(in_size)\n",
        "    #print('x size:',x.size())\n",
        "    x = self.fc(x)\n",
        "    return  nn.functional.softmax(x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZtnKVk9Tbev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "eb0aa87c-37d9-4eb3-e3cc-49db377eecdd"
      },
      "source": [
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "if use_cuda and cuda.is_available():\n",
        "  device = 'cuda' \n",
        "  model.cuda()\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(f'Training MNIST　Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr_rate, momentum=0.5)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=320, out_features=10, bias=True)\n",
            ")\n",
            "Training MNIST　Model on cpu\n",
            "============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8YOjA8AThHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx*len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3qFVBqGUkMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data,volatile=True).to(device), Variable(target).to(device)\n",
        "    output = model(data)\n",
        "    # sum up batch loss \n",
        "    test_loss += criterion(output, target).item()   \n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)  \n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yD5x1nsWeHv",
        "colab_type": "code",
        "outputId": "61ff7510-bf16-4a2e-de5d-6e7b900ab6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 5):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        \n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.302077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.301776\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.302138\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.303964\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.301830\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.303806\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.300904\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.301658\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.302811\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.303802\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.300665\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.301226\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.302220\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.301900\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.301149\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.301862\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.301331\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.300623\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.299819\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.299176\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.299600\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.301137\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.299813\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.300447\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.298950\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.300066\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.299453\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.299978\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.297740\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.299936\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.300373\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.300384\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.299466\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.298931\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.297129\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.298566\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.296570\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.295831\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.299389\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.297513\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.297442\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.298645\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.298210\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.298329\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.297925\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.299021\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.299319\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.299029\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.297007\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.295866\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.296441\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.292902\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.298956\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.295291\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.295391\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.294182\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.295410\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.291704\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.294067\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.295653\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.294276\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.294962\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.292104\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.294516\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.292234\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.289187\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.295297\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.291239\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.290969\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.288576\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.289070\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.287152\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.290897\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.293569\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.287951\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.285300\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.285935\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.286222\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.278534\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.279102\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.283210\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.274842\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.275882\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.272855\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 2.264760\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 2.265838\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 2.273395\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 2.256869\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 2.247492\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 2.195329\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 2.206418\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 2.246786\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 2.236431\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 2.187597\n",
            "Training time: 0m 25s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 22/10000 (0%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 34/10000 (0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 55/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 70/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 90/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 109/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 123/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 136/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 149/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 160/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 177/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 196/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 216/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 230/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 248/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 269/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 287/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 298/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 315/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 333/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 353/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 366/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 380/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 397/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 411/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 428/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 448/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 467/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 486/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 504/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 517/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 538/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 548/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 561/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 579/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 593/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 614/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 634/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 644/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 660/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 672/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 691/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 710/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 727/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 742/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 757/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 773/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 792/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 806/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 820/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 843/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 861/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 874/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 895/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 908/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 919/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 940/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 960/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 976/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 992/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1008/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1020/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1036/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1058/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1070/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1093/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1108/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1127/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1140/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1151/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1167/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1183/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1200/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1216/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1228/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1243/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1262/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1279/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1297/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1314/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1330/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1342/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1361/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1382/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1393/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1409/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1430/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1442/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1459/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1473/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1487/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1501/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1518/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1543/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1555/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1574/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1584/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1599/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1616/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1632/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1650/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1666/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1683/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1699/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1715/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1733/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1750/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1764/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1786/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1798/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1817/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1830/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1848/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1864/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1879/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1893/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1907/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1919/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1938/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1955/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1974/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1984/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2004/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2022/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2035/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2052/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2070/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2083/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2098/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2110/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2131/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2145/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2162/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2180/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2200/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2213/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2233/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2249/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2266/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2277/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2297/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2313/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2326/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2339/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2355/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2374/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2396/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2412/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2430/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2444/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2466/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2481/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2491/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2510/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2521/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2539/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2543/10000 (25%)\n",
            "Testing time: 0m 27s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 2.233432\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 2.195886\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 2.160950\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 2.200148\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 2.173520\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 2.121171\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 2.142802\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 2.118823\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 2.143240\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 2.021652\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 2.079644\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 2.034269\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 1.977226\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 2.059561\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 1.983910\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 1.965167\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 1.926918\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 1.877866\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 1.936936\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 1.867709\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 1.885642\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 1.924471\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 1.992350\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 1.952612\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 1.865313\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 1.866844\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 1.821036\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 1.931314\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 1.826155\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 1.862001\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 1.938064\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 1.890160\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 1.810848\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 1.883420\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 1.856912\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 1.820526\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 1.804971\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 1.707068\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 1.870500\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 1.763410\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 1.757932\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 1.690352\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 1.789610\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 1.791598\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 1.877167\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 1.876798\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 1.739018\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 1.738063\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 1.856930\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 1.733530\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 1.732269\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 1.701396\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 1.775008\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 1.741747\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 1.848434\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 1.815894\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 1.744009\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 1.704268\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 1.800508\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 1.770496\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 1.750667\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 1.760081\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 1.768789\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 1.878973\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 1.772127\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 1.811555\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 1.691481\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 1.724513\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 1.756474\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 1.684999\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 1.726484\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 1.624875\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 1.813219\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 1.685721\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 1.725117\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 1.797800\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 1.732386\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 1.773339\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 1.750572\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 1.739110\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 1.619410\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 1.721009\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 1.741886\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 1.669390\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 1.726779\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 1.638223\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 1.667706\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 1.700299\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 1.786085\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 1.765259\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 1.841152\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 1.718423\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 1.722007\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 1.674571\n",
            "Training time: 0m 25s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 52/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 102/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 144/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 195/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 241/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 284/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 335/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 377/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 422/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 467/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 512/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 560/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 611/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 661/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 710/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 756/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 805/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 847/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 894/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 931/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 975/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1024/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1067/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1115/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1161/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1212/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1253/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1299/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1353/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1397/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1443/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1490/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1534/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1576/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1621/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1671/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1726/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1771/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1824/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1869/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1909/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1955/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2004/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2048/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2095/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2142/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2188/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2240/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2286/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2334/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2386/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2436/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2480/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2528/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2576/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2616/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2662/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2711/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2756/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2791/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2831/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2874/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2924/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2969/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3012/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3059/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3101/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3146/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3187/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3231/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3281/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3334/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3385/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3435/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3478/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3520/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3570/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3619/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3673/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3720/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3768/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3814/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3866/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3920/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3966/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4020/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4072/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4117/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4163/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4211/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4263/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4304/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4348/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4395/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4446/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4499/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4543/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4593/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4645/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4694/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4742/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4786/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4834/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4879/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4936/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4987/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5037/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5082/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5133/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5183/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5237/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5285/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5340/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5390/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5442/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5491/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5540/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5589/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5640/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5688/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5740/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5787/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5835/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5887/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5935/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5983/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6034/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6083/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6132/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6181/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6235/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6286/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6339/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6389/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6443/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6490/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6541/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6594/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6645/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6699/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6751/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6800/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6848/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6899/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6954/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6999/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7051/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7101/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7152/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7199/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7248/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7294/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7339/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7385/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7431/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7482/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7494/10000 (75%)\n",
            "Testing time: 0m 27s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 1.629200\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 1.746205\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 1.709763\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 1.765821\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 1.748042\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 1.732419\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 1.749038\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 1.624505\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 1.778481\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 1.769358\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 1.732737\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 1.654340\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 1.740201\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 1.716317\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 1.754585\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 1.730321\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 1.638495\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 1.735865\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 1.821231\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 1.687993\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 1.724132\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 1.640656\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 1.715805\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 1.696339\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 1.762587\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 1.670091\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 1.738371\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 1.678825\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 1.714169\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 1.776969\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 1.743342\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 1.786965\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 1.749537\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 1.707112\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 1.608963\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 1.756821\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 1.633448\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 1.760457\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 1.696745\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 1.648496\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 1.649700\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 1.778648\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 1.671414\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 1.775471\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 1.715370\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 1.776305\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 1.729956\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 1.687256\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 1.644040\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 1.654183\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 1.745115\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 1.682287\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 1.775070\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 1.688850\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 1.741152\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 1.679935\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 1.823596\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 1.688024\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 1.668864\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 1.734435\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 1.672000\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 1.782835\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 1.631613\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 1.647020\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 1.701294\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 1.617896\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 1.596175\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 1.655627\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 1.619365\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 1.726253\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 1.669549\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 1.616282\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 1.664911\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 1.659603\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 1.636741\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 1.714957\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 1.664027\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 1.628720\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 1.654161\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 1.608395\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 1.706565\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 1.645434\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 1.692038\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 1.636938\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 1.689508\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 1.638409\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 1.687878\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 1.645838\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 1.595573\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 1.635305\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 1.653790\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 1.662159\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 1.735427\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 1.686086\n",
            "Training time: 0m 25s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 53/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 107/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 158/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 211/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 261/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 309/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 366/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 416/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 466/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 517/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 569/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 619/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 677/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 734/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 785/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 836/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 889/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 934/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 986/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1033/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1084/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1139/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1187/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1238/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1296/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1350/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1396/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1449/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1505/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1554/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1605/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1655/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1706/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1757/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1808/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1862/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1918/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1967/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2023/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2073/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2120/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2172/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2224/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2275/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2329/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2379/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2429/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2487/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2536/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2589/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2649/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2704/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2755/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2806/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2864/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2916/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2968/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3023/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3071/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3112/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3158/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3206/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3259/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3307/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3362/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3413/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3462/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3510/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3564/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3614/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3671/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3725/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3783/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3838/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3890/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3941/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3993/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4044/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4103/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4156/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4209/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4262/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4317/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4374/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4431/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4489/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4545/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4593/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4646/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4699/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4757/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4805/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4855/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4909/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4964/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5017/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5068/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5125/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5182/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5238/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5294/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5347/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5398/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5453/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5515/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5569/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5623/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5675/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5731/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5787/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5847/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5903/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5961/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6015/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6072/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6129/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6180/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6234/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6291/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6345/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6404/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6461/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6511/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6567/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6622/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6678/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6734/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6788/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6844/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6900/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6957/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7014/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7071/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7127/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7186/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7240/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7298/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7355/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7412/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7473/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7526/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7578/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7634/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7690/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7748/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7803/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7860/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7918/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7971/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8026/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8079/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8128/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8176/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8226/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8276/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8331/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8343/10000 (83%)\n",
            "Testing time: 0m 27s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 1.618642\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 1.597028\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 1.609780\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 1.683299\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 1.692037\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 1.652235\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 1.612667\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 1.621736\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 1.732186\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 1.652718\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 1.699840\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 1.691364\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 1.605814\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 1.675667\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 1.599945\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 1.646809\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 1.706615\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 1.625201\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 1.627692\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 1.760546\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 1.594186\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 1.656018\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 1.620328\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 1.620899\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 1.634949\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 1.623088\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 1.616665\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 1.656954\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 1.666973\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 1.648694\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 1.667270\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 1.652866\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 1.668288\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 1.684748\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 1.682063\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 1.622568\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 1.560963\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 1.617670\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 1.637151\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 1.566168\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 1.717248\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 1.696287\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 1.740003\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 1.658847\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 1.616447\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 1.642508\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 1.617531\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 1.653340\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 1.620885\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 1.622726\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 1.673892\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 1.656267\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 1.601888\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 1.581685\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 1.616370\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 1.675183\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 1.595231\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 1.601500\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 1.570268\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 1.636232\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 1.743970\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 1.591110\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 1.622316\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 1.578297\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 1.576197\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 1.629752\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 1.625820\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 1.724938\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 1.725150\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 1.614990\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 1.571499\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 1.674512\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 1.576217\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 1.578085\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 1.604185\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 1.695445\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 1.636561\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 1.598521\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 1.538907\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 1.638957\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 1.580952\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 1.593731\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 1.609694\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 1.613714\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 1.673625\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 1.691607\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 1.579982\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 1.631750\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 1.611894\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 1.591107\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 1.622560\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 1.652494\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 1.592930\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 1.600583\n",
            "Training time: 0m 25s\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 55/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 114/10000 (1%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 167/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 221/10000 (2%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 276/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 324/10000 (3%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 379/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 432/10000 (4%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 482/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 534/10000 (5%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 587/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 636/10000 (6%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 694/10000 (7%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 751/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 802/10000 (8%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 855/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 907/10000 (9%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 954/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1007/10000 (10%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1055/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1107/10000 (11%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1162/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1214/10000 (12%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1267/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1325/10000 (13%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1379/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1428/10000 (14%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1481/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1537/10000 (15%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1586/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1641/10000 (16%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1692/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1743/10000 (17%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1796/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1849/10000 (18%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1904/10000 (19%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 1961/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2011/10000 (20%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2068/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2119/10000 (21%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2167/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2219/10000 (22%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2272/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2324/10000 (23%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2381/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2433/10000 (24%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2483/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2542/10000 (25%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2592/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2646/10000 (26%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2706/10000 (27%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2763/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2816/10000 (28%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2868/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2927/10000 (29%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 2979/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3031/10000 (30%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3087/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3139/10000 (31%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3180/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3228/10000 (32%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3277/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3334/10000 (33%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3384/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3441/10000 (34%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3494/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3544/10000 (35%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3593/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3648/10000 (36%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3701/10000 (37%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3757/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3811/10000 (38%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3870/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3927/10000 (39%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 3981/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4033/10000 (40%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4086/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4139/10000 (41%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4200/10000 (42%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4255/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4310/10000 (43%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4364/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4420/10000 (44%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4477/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4534/10000 (45%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4593/10000 (46%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4652/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4703/10000 (47%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4758/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4811/10000 (48%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4870/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4924/10000 (49%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 4977/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5032/10000 (50%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5087/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5142/10000 (51%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5195/10000 (52%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5252/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5309/10000 (53%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5365/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5423/10000 (54%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5477/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5528/10000 (55%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5582/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0001, Accuracy: 5644/10000 (56%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5701/10000 (57%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5757/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5810/10000 (58%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5866/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5922/10000 (59%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 5980/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6036/10000 (60%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6094/10000 (61%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6151/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6208/10000 (62%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6265/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6316/10000 (63%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6371/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6429/10000 (64%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6482/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6540/10000 (65%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6598/10000 (66%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6652/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6709/10000 (67%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6764/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6820/10000 (68%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6876/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6931/10000 (69%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 6989/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7047/10000 (70%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7105/10000 (71%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7162/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7220/10000 (72%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7276/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7333/10000 (73%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7387/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7445/10000 (74%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7502/10000 (75%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7558/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7619/10000 (76%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7672/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7728/10000 (77%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7785/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7843/10000 (78%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7901/10000 (79%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 7955/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8012/10000 (80%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8070/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8122/10000 (81%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8178/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8233/10000 (82%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8284/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8333/10000 (83%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8384/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8437/10000 (84%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8492/10000 (85%)\n",
            "===========================\n",
            "Test set: Average loss: 0.0002, Accuracy: 8504/10000 (85%)\n",
            "Testing time: 0m 27s\n",
            "Total Time: 1m 48s\n",
            "Model was trained on cpu!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7BzB4AU6tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}